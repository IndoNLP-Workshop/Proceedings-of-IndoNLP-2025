@InProceedings{parida-EtAl:0:myconference,
  author    = {Parida, Shantipriya  and  Sahoo, Shashikanta  and  Sekhar, Sambit  and  Sahoo, Kalyanamalini  and  Kotwal, Ketan  and  Khosla, Sonal  and  Dash, Satya Ranjan  and  Bose, Aneesh  and  Kohli, Guneet Singh  and  Lenka, Smruti Smita  and  Bojar, Ond≈ôej},
  title     = {OVQA: A Dataset for Visual Question Answering and Multimodal Research in Odia Language},
  booktitle      = {TOBEFILLED-Proceedings of the Second Workshop on Building Educational Applications Using NLP},
  month          = {TOBEFILLED-June},
  year           = {TOBEFILLED-1},
  address        = {TOBEFILLED-Ann Arbor, Michigan},
  publisher      = {Association for Computational Linguistics},
  pages     = {85--92},
  abstract  = {This paper introduces OVQA, the first multimodal dataset designed for visual question-answering (VQA), visual question elicitation (VQE), and multimodal research for the low-resource Odia language. The dataset was created by manually translating 6,149 English question-answer pairs, each associated with 6,149 unique images from the Visual Genome dataset. This effort resulted in 27,809 English-Odia parallel sentences, ensuring a semantic match with the corresponding visual information. Several baseline experiments were conducted on the dataset, including visual question answering and visual question elicitation. The dataset is the first VQA dataset for the low-resource Odia language and will be released for multimodal research purposes and also help researchers extend for other low-resource languages.},
  url       = {https://aclanthology.org/TOBEFILLED-1.myconference-1.9}
}

